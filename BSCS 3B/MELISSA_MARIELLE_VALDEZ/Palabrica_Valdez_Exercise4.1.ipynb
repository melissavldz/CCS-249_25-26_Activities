{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f16cffb",
   "metadata": {},
   "source": [
    "# Exercise: Naïve Bayes Implementation\n",
    "\n",
    "Melissa Marielle Valdez BSCS 3B - AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b16fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ef1af",
   "metadata": {},
   "source": [
    "## Task 1: Manual Naïve Bayes Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c08a22",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bf50755",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    (\"Free money now!!!\", 'SPAM'),\n",
    "    (\"Hi mom, how are you?\", 'HAM'),\n",
    "    (\"Lowest price for your meds\", 'SPAM'),\n",
    "    (\"Are we still on for dinner?\", 'HAM'),\n",
    "    (\"Win a free iPhone today\", 'SPAM'),\n",
    "    (\"Let's catch up tomorrow at the office\", 'HAM'),\n",
    "    (\"Meeting at 3 PM tomorrow\", 'HAM'),\n",
    "    (\"Get 50% off, limited time!\", 'SPAM'),\n",
    "    (\"Team meeting in the office\", 'HAM'),\n",
    "    (\"Click here for prizes!\", 'SPAM'),\n",
    "    (\"Can you send the report?\", 'HAM'),\n",
    "]\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text.split()  # Tokenize into words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bc1f5",
   "metadata": {},
   "source": [
    "### Part 1.a: Bag of Words (Word Frequency)\n",
    "Build the vocabulary and generate word frequency counts for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27120c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1.a: BAG OF WORDS (Word Frequencies) ---\n",
      "\n",
      "SPAM - Word Frequencies:\n",
      "Word            |      Count\n",
      "----------------------------\n",
      "free            |          2\n",
      "for             |          2\n",
      "money           |          1\n",
      "now             |          1\n",
      "lowest          |          1\n",
      "price           |          1\n",
      "your            |          1\n",
      "meds            |          1\n",
      "win             |          1\n",
      "a               |          1\n",
      "iphone          |          1\n",
      "today           |          1\n",
      "get             |          1\n",
      "50              |          1\n",
      "off             |          1\n",
      "limited         |          1\n",
      "time            |          1\n",
      "click           |          1\n",
      "here            |          1\n",
      "prizes          |          1\n",
      "\n",
      "HAM - Word Frequencies:\n",
      "Word            |      Count\n",
      "----------------------------\n",
      "the             |          3\n",
      "are             |          2\n",
      "you             |          2\n",
      "tomorrow        |          2\n",
      "at              |          2\n",
      "office          |          2\n",
      "meeting         |          2\n",
      "hi              |          1\n",
      "mom             |          1\n",
      "how             |          1\n",
      "we              |          1\n",
      "still           |          1\n",
      "on              |          1\n",
      "for             |          1\n",
      "dinner          |          1\n",
      "lets            |          1\n",
      "catch           |          1\n",
      "up              |          1\n",
      "3               |          1\n",
      "pm              |          1\n",
      "team            |          1\n",
      "in              |          1\n",
      "can             |          1\n",
      "send            |          1\n",
      "report          |          1\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary and word frequency counts\n",
    "vocab = set()\n",
    "class_counts = defaultdict(Counter)\n",
    "doc_counts = defaultdict(int)\n",
    "total_docs = len(docs)\n",
    "\n",
    "for text, label in docs:\n",
    "    tokens = tokenize(text)\n",
    "    vocab.update(tokens)\n",
    "    class_counts[label].update(tokens)\n",
    "    doc_counts[label] += 1\n",
    "\n",
    "V = len(vocab)\n",
    "N_spam = sum(class_counts['SPAM'].values())\n",
    "N_ham = sum(class_counts['HAM'].values())\n",
    "\n",
    "# Display Bag of Words (Word Frequencies)\n",
    "print(\"\\n--- PART 1.a: BAG OF WORDS (Word Frequencies) ---\")\n",
    "print(\"\\nSPAM - Word Frequencies:\")\n",
    "print(f\"{'Word':<15} | {'Count':>10}\")\n",
    "print(\"-\" * 28)\n",
    "for word, count in sorted(class_counts['SPAM'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word:<15} | {count:>10}\")\n",
    "print(\"\\nHAM - Word Frequencies:\")\n",
    "print(f\"{'Word':<15} | {'Count':>10}\")\n",
    "print(\"-\" * 28)\n",
    "for word, count in sorted(class_counts['HAM'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word:<15} | {count:>10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69ff62",
   "metadata": {},
   "source": [
    "### Part 1.b: Prior Probabilities for HAM and SPAM\n",
    "Calculate P(SPAM) and P(HAM) from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6544bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1.b: PRIOR PROBABILITIES ---\n",
      "\n",
      "Metric                    |            SPAM |             HAM\n",
      "------------------------------------------------------------\n",
      "Document Count            |               5 |               6\n",
      "Token Count               |              22 |              33\n",
      "Prior Probability         |          0.4545 |          0.5455\n",
      "\n",
      "Vocabulary size V = 44\n",
      "Total documents = 11\n"
     ]
    }
   ],
   "source": [
    "prior_spam = doc_counts['SPAM'] / total_docs\n",
    "prior_ham = doc_counts['HAM'] / total_docs\n",
    "\n",
    "print(\"\\n--- PART 1.b: PRIOR PROBABILITIES ---\")\n",
    "print(f\"\\n{'Metric':<25} | {'SPAM':>15} | {'HAM':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Document Count':<25} | {doc_counts['SPAM']:>15} | {doc_counts['HAM']:>15}\")\n",
    "print(f\"{'Token Count':<25} | {N_spam:>15} | {N_ham:>15}\")\n",
    "print(f\"{'Prior Probability':<25} | {prior_spam:>15.4f} | {prior_ham:>15.4f}\")\n",
    "print(f\"\\nVocabulary size V = {V}\")\n",
    "print(f\"Total documents = {total_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c35dc4",
   "metadata": {},
   "source": [
    "### Part 1.c: Likelihood of Tokens with Respect to Class\n",
    "Calculate P(token|class) for tokens in vocabulary with Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f934c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1.c: LIKELIHOOD OF TOKENS ---\n",
      "\n",
      "Token Likelihood Table:\n",
      "Token        |   P(token|SPAM) |    P(token|HAM)\n",
      "------------------------------------------------\n",
      "3            |          0.0152 |          0.0260\n",
      "50           |          0.0303 |          0.0130\n",
      "a            |          0.0303 |          0.0130\n",
      "are          |          0.0152 |          0.0390\n",
      "at           |          0.0152 |          0.0390\n",
      "can          |          0.0152 |          0.0260\n",
      "catch        |          0.0152 |          0.0260\n",
      "click        |          0.0303 |          0.0130\n",
      "dinner       |          0.0152 |          0.0260\n",
      "for          |          0.0455 |          0.0260\n",
      "free         |          0.0455 |          0.0130\n",
      "get          |          0.0303 |          0.0130\n",
      "here         |          0.0303 |          0.0130\n",
      "hi           |          0.0152 |          0.0260\n",
      "how          |          0.0152 |          0.0260\n",
      "in           |          0.0152 |          0.0260\n",
      "iphone       |          0.0303 |          0.0130\n",
      "lets         |          0.0152 |          0.0260\n",
      "limited      |          0.0303 |          0.0130\n",
      "lowest       |          0.0303 |          0.0130\n",
      "meds         |          0.0303 |          0.0130\n",
      "meeting      |          0.0152 |          0.0390\n",
      "mom          |          0.0152 |          0.0260\n",
      "money        |          0.0303 |          0.0130\n",
      "now          |          0.0303 |          0.0130\n",
      "off          |          0.0303 |          0.0130\n",
      "office       |          0.0152 |          0.0390\n",
      "on           |          0.0152 |          0.0260\n",
      "pm           |          0.0152 |          0.0260\n",
      "price        |          0.0303 |          0.0130\n",
      "prizes       |          0.0303 |          0.0130\n",
      "report       |          0.0152 |          0.0260\n",
      "send         |          0.0152 |          0.0260\n",
      "still        |          0.0152 |          0.0260\n",
      "team         |          0.0152 |          0.0260\n",
      "the          |          0.0152 |          0.0519\n",
      "time         |          0.0303 |          0.0130\n",
      "today        |          0.0303 |          0.0130\n",
      "tomorrow     |          0.0152 |          0.0390\n",
      "up           |          0.0152 |          0.0260\n",
      "we           |          0.0152 |          0.0260\n",
      "win          |          0.0303 |          0.0130\n",
      "you          |          0.0152 |          0.0390\n",
      "your         |          0.0303 |          0.0130\n"
     ]
    }
   ],
   "source": [
    "def likelihood(token, label):\n",
    "    count = class_counts[label][token]\n",
    "    N = sum(class_counts[label].values())\n",
    "    return (count + 1) / (N + V)   # Laplace smoothing\n",
    "\n",
    "# Display all likelihoods in column format\n",
    "print(\"\\n--- PART 1.c: LIKELIHOOD OF TOKENS ---\")\n",
    "print(\"\\nToken Likelihood Table:\")\n",
    "print(f\"{'Token':<12} | {'P(token|SPAM)':>15} | {'P(token|HAM)':>15}\")\n",
    "print(\"-\" * 48)\n",
    "for token in sorted(vocab):\n",
    "    p_spam = likelihood(token, 'SPAM')\n",
    "    p_ham = likelihood(token, 'HAM')\n",
    "    print(f\"{token:<12} | {p_spam:>15.4f} | {p_ham:>15.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1027155",
   "metadata": {},
   "source": [
    "### Part 1.d: Classification of Test Sentences\n",
    "Use the trained manual Naïve Bayes classifier to classify the following test sentences:\n",
    "- i. \"Limited offer, click here!\"\n",
    "- ii. \"Meeting at 2 PM with the manager.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef17c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1.d: CLASSIFICATION OF TEST SENTENCES ---\n",
      "\n",
      "Sentence: Limited offer, click here!\n",
      "  Predicted: SPAM\n",
      "  P(SPAM) = 0.9251\n",
      "  P(HAM)  = 0.0749\n",
      "  logP(SPAM|s) = -15.468\n",
      "  logP(HAM|s)  = -17.981\n",
      "  Confidence: 92.51%\n",
      "\n",
      "Sentence: Meeting at 2 PM with the manager.\n",
      "  Predicted: HAM\n",
      "  P(SPAM) = 0.0329\n",
      "  P(HAM)  = 0.9671\n",
      "  logP(SPAM|s) = -30.116\n",
      "  logP(HAM|s)  = -26.736\n",
      "  Confidence: 96.71%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    log_spam = math.log(prior_spam)\n",
    "    log_ham  = math.log(prior_ham)\n",
    "    for t in tokens:\n",
    "        log_spam += math.log(likelihood(t,'SPAM'))\n",
    "        log_ham  += math.log(likelihood(t,'HAM'))\n",
    "    return ('SPAM' if log_spam > log_ham else 'HAM', log_spam, log_ham)\n",
    "\n",
    "tests = [\"Limited offer, click here!\", \"Meeting at 2 PM with the manager.\"]\n",
    "print(\"\\n--- PART 1.d: CLASSIFICATION OF TEST SENTENCES ---\")\n",
    "print()\n",
    "for s in tests:\n",
    "    predicted_class, log_spam, log_ham = classify(s)\n",
    "    \n",
    "    # Calculate confidence from log probabilities\n",
    "    prob_spam = math.exp(log_spam) / (math.exp(log_spam) + math.exp(log_ham))\n",
    "    prob_ham = math.exp(log_ham) / (math.exp(log_spam) + math.exp(log_ham))\n",
    "    confidence = max(prob_spam, prob_ham) * 100\n",
    "    \n",
    "    print(f\"Sentence: {s}\")\n",
    "    print(f\"  Predicted: {predicted_class}\")\n",
    "    print(f\"  P(SPAM) = {prob_spam:.4f}\")\n",
    "    print(f\"  P(HAM)  = {prob_ham:.4f}\")\n",
    "    print(f\"  logP(SPAM|s) = {log_spam:.3f}\")\n",
    "    print(f\"  logP(HAM|s)  = {log_ham:.3f}\")\n",
    "    print(f\"  Confidence: {confidence:.2f}%\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3c166",
   "metadata": {},
   "source": [
    "## Task 2: Scikit-Learn Multinomial Naïve Bayes\n",
    "\n",
    "Train a Multinomial Naïve Bayes classifier using scikit-learn and classify the same test sentences:\n",
    "- i. \"Limited offer, click here!\"\n",
    "- ii. \"Meeting at 2 PM with the manager.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "133d0e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TASK 2: SCIKIT-LEARN PREDICTIONS ---\n",
      "\n",
      "Sentence: Limited offer, click here!\n",
      "  Predicted: SPAM\n",
      "  P(HAM)  = 0.0838\n",
      "  P(SPAM) = 0.9162\n",
      "  Confidence: 91.62%\n",
      "\n",
      "Sentence: Meeting at 2 PM with the manager.\n",
      "  Predicted: HAM\n",
      "  P(HAM)  = 0.9781\n",
      "  P(SPAM) = 0.0219\n",
      "  Confidence: 97.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn version\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "train_texts = [t for t,_ in docs]\n",
    "train_labels = [l for _,l in docs]\n",
    "vec = CountVectorizer(lowercase=True, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "X = vec.fit_transform(train_texts)\n",
    "clf = MultinomialNB(alpha=1.0)   # Laplace smoothing\n",
    "clf.fit(X, train_labels)\n",
    "\n",
    "X_test = vec.transform(tests)\n",
    "preds = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n--- TASK 2: SCIKIT-LEARN PREDICTIONS ---\")\n",
    "print()\n",
    "for i, sentence in enumerate(tests):\n",
    "    predicted_class = preds[i]\n",
    "    prob_ham = probs[i][0] if clf.classes_[0] == 'HAM' else probs[i][1]\n",
    "    prob_spam = probs[i][1] if clf.classes_[0] == 'HAM' else probs[i][0]\n",
    "    confidence = max(probs[i]) * 100\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"  Predicted: {predicted_class}\")\n",
    "    print(f\"  P(HAM)  = {prob_ham:.4f}\")\n",
    "    print(f\"  P(SPAM) = {prob_spam:.4f}\")\n",
    "    print(f\"  Confidence: {confidence:.2f}%\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
